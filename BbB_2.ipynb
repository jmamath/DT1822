{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BbB_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SJUrvUZk_w-",
        "colab_type": "text"
      },
      "source": [
        "#Downloading required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuzK4avxRUe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe4aaee1-4c21-4e53-c1a0-79b7fa64e807"
      },
      "source": [
        "!pip install gpyopt\n",
        "!pip install pycuda\n",
        "!pip install pyro-ppl==0.3.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpyopt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/40/ca8f080d74d9f4e29069faa944fcfb083e8693b6daaba0f1e4bc65c88650/GPyOpt-1.2.5.tar.gz (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 711kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from gpyopt) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from gpyopt) (1.3.0)\n",
            "Collecting GPy>=1.8 (from gpyopt)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/4418ee1db50c6e917e399db841c30b95d3c242555272e85473404ab06377/GPy-1.9.8.tar.gz (989kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 679kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from GPy>=1.8->gpyopt) (1.12.0)\n",
            "Collecting paramz>=0.9.0 (from GPy>=1.8->gpyopt)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/37/4abbeb78d30f20d3402887f46e6e9f3ef32034a9dea65d243654c82c8553/paramz-0.9.5.tar.gz (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/dist-packages (from paramz>=0.9.0->GPy>=1.8->gpyopt) (4.4.0)\n",
            "Building wheels for collected packages: gpyopt, GPy, paramz\n",
            "  Building wheel for gpyopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/1d/87/dc02440831ba986b1547dd11a7dcd44e893b0527083066d869\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/ee/cd/1c4dd7df63246b1e8de58af6d4457b7aed13509fdc0c918a13\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/4a/0e/6e0dc85541825f991c431619e25b870d4b812c911214690cf8\n",
            "Successfully built gpyopt GPy paramz\n",
            "Installing collected packages: paramz, GPy, gpyopt\n",
            "Successfully installed GPy-1.9.8 gpyopt-1.2.5 paramz-0.9.5\n",
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/35/130ac8867b30f9c6ae699b689633a803a73787533f41e52afcf28b75bd81/pycuda-2019.1.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 605kB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2 (from pycuda)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/96/00416762a3eda8876a17d007df4a946f46b2e4ee1057e0b9714926472ef8/pytools-2019.1.1.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 19.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest>=2 in /usr/local/lib/python3.6/dist-packages (from pycuda) (3.6.4)\n",
            "Requirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.0)\n",
            "Collecting appdirs>=1.4.0 (from pycuda)\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Collecting mako (from pycuda)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/29/8016763284d8fab844224f7cc5675cb4a388ebda0eb5a403260187e48435/Mako-1.0.13.tar.gz (460kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.16.4)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (7.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (41.0.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (1.8.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (19.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools, mako\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/f5/72/73296e7845a1ddd8769ef5baa6a8bb05bbd8baedc18184b0d1\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/df/0b/75ac4572aaa93e3eba6a58472635d0fda907f5f4cf884a3a0c\n",
            "  Building wheel for mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/0e/59/0e7f24103d1ebce045037aa17b75548a8387f5e7d2d0011fdc\n",
            "Successfully built pycuda pytools mako\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.3 mako-1.0.13 pycuda-2019.1.1 pytools-2019.1.1\n",
            "Collecting pyro-ppl==0.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/df/9d49d1fa8d4e47d473c801bf22f9d2a433a0e61c609bedc5681ae0e22e76/pyro-ppl-0.3.2.tar.gz (230kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 795kB/s \n",
            "\u001b[?25hRequirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==0.3.2) (0.5.5)\n",
            "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==0.3.2) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==0.3.2) (1.16.4)\n",
            "Collecting opt_einsum>=2.3.2 (from pyro-ppl==0.3.2)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 20.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==0.3.2) (1.12.0)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==0.3.2) (1.1.0)\n",
            "Requirement already satisfied: tqdm>=4.28 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl==0.3.2) (4.28.1)\n",
            "Building wheels for collected packages: pyro-ppl, opt-einsum\n",
            "  Building wheel for pyro-ppl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2f/21/9f840307c05be374f101b0e4aee2db0596437fe2ade0334c2f\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built pyro-ppl opt-einsum\n",
            "Installing collected packages: opt-einsum, pyro-ppl\n",
            "Successfully installed opt-einsum-2.3.2 pyro-ppl-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jmDJT0elIZb",
        "colab_type": "text"
      },
      "source": [
        "#Loading libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NJD0FUWRg20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import GPyOpt\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import pyro\n",
        "from pyro.distributions import Normal, Delta\n",
        "from diag_normal_mixture import MixtureOfDiagNormals\n",
        "from pyro.infer import  SVI , Trace_ELBO , EmpiricalMarginal, TracePredictive\n",
        "from pyro.optim import Adam\n",
        "from torch.distributions import constraints\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import fileinput\n",
        "import pycuda.driver as cuda\n",
        "import requests\n",
        "from functools import partial\n",
        "cuda.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-YxmdzlQwx",
        "colab_type": "text"
      },
      "source": [
        "#Loading data\n",
        "\n",
        "This is mostly boiler plate code, except for the boston and yacht datasets, every data is read directly from the source on github. For the case of boston and yacht datasets, we download them locally to fix some issues concerning the separator between each columns.\n",
        "\n",
        "Each dataset call is then associated to a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC6tG7XWRr0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Loading data \n",
        "\"\"\" \n",
        "\n",
        "\"\"\"\n",
        "boston_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/boston_housing.txt\"\n",
        "concrete_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/concrete.txt\"\n",
        "energy_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/energy_heating_load.txt\"\n",
        "kin8_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/kin8nm.txt\"\n",
        "naval_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/naval_compressor_decay.txt\"\n",
        "power_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/power.txt\"\n",
        "protein_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/protein.txt\"\n",
        "wine_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/wine.txt\"\n",
        "yacht_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/yacht.txt\"\n",
        "year_prediction_url = \"https://media.githubusercontent.com/media/probabilisticai/dt8122/master/datasets/year_prediction_msd.txt\"\n",
        "\n",
        "\n",
        "def boston():\n",
        "  r = requests.get(boston_url)\n",
        "  with open('/content/boston.txt', 'wb') as f:  \n",
        "    f.write(r.content)\n",
        "  with fileinput.FileInput(\"boston.txt\", inplace=True, backup='.bak') as file:\n",
        "    for line in file:\n",
        "        print(line.replace(\"   \", \" \"), end='')\n",
        "  bos = [str(i) for i in range(14)]\n",
        "  boston = pd.read_csv('boston.txt', sep=\"  | \",names = bos, header=None)\n",
        "  print(\"Boston dataset - {} examples - 14 columns\".format(len(boston)))\n",
        "  print(\"Number of Nan :{}\".format(boston.isna().sum().sum())) \n",
        "  return boston, 14, \"Boston Housing\"\n",
        "\n",
        "def concrete():\n",
        "  conc = [str(i) for i in range(9)]\n",
        "  concrete = pd.read_csv(concrete_url, sep=\" |\\t\",names = conc, header=None)\n",
        "  print(\"Concrete dataset - {} examples - 9 columns\".format(len(concrete)))\n",
        "  print(\"Number of Nan :{}\".format(concrete.isna().sum().sum())) \n",
        "  return concrete, 9, \"Concrete Strength\"\n",
        "\n",
        "def energy():  \n",
        "  ener = [str(i) for i in range(9)]\n",
        "  energy = pd.read_csv(energy_url, sep=\" |\\t\",names = ener, header=None)\n",
        "  print(\"Energy dataset - {} examples - 9 columns\".format(len(energy)))\n",
        "  print(\"Number of Nan :{}\".format(energy.isna().sum().sum())) \n",
        "  return energy, 9, \"Energy Efficiency\"\n",
        "\n",
        "def kin8nm():\n",
        "  kin8 = [str(i) for i in range(9)]\n",
        "  kin8nm = pd.read_csv(kin8_url, sep=\"   |  \",names = kin8, header=None)\n",
        "  print(\"Kin8nm dataset - {} examples - 10 columns\".format(len(kin8nm)))\n",
        "  print(\"Number of Nan :{}\".format(kin8nm.isna().sum().sum())) \n",
        "  return kin8nm, 9, \"Kin8nm\"\n",
        "\n",
        "def naval():\n",
        "  nav = [str(i) for i in range(17)]\n",
        "  naval = pd.read_csv(naval_url, sep=\" |\\t\", names = nav, header=None)\n",
        "  print(\"Naval dataset - {} examples - 17 columns\".format(len(naval)))\n",
        "  print(\"Number of Nan :{}\".format(naval.isna().sum().sum())) \n",
        "  return naval, 17, \"Naval Propulsion\"\n",
        "\n",
        "def power():\n",
        "  pow = [str(i) for i in range(5)]\n",
        "  power = pd.read_csv(power_url, sep=\" |\\t\", names = pow, header=None)\n",
        "  print(\"Power dataset - {} examples - 5 columns\".format(len(power)))\n",
        "  print(\"Number of Nan :{}\".format(power.isna().sum().sum())) \n",
        "  return power, 5, \"Power Plant\"\n",
        "\n",
        "def protein():\n",
        "  prot = [str(i) for i in range(10)]\n",
        "  protein = pd.read_csv(protein_url, sep=\" |\\t\", names = prot, header=None)\n",
        "  print(\"Protein dataset - {} examples - 10 columns\".format(len(protein)))\n",
        "  print(\"Number of Nan :{}\".format(protein.isna().sum().sum())) \n",
        "  return protein, 10, \"Protein Structure\"\n",
        "\n",
        "def wine():\n",
        "  w = [str(i) for i in range(12)]\n",
        "  wine = pd.read_csv(wine_url, sep=\" |\\t\", names = w, header=None)\n",
        "  print(\"Wine dataset - {} examples - 12 columns\".format(len(wine)))\n",
        "  print(\"Number of Nan :{}\".format(wine.isna().sum().sum())) \n",
        "  return wine, 12, \"Wine Quality Red\"\n",
        "\n",
        "def yacht(): \n",
        "  r = requests.get(yacht_url)\n",
        "  with open('/content/yacht.txt', 'wb') as f:  \n",
        "      f.write(r.content) \n",
        "  with fileinput.FileInput(\"yacht.txt\", inplace=True, backup='.bak') as file:\n",
        "    for line in file:\n",
        "        print(line.replace(\"  \", \" \"), end='')\n",
        "  ya = [str(i) for i in range(7)]        \n",
        "  yacht = pd.read_csv('yacht.txt', sep=\" \", names = ya, header=None)\n",
        "  print(\"Yacht dataset - {} examples - 7 columns\".format(len(yacht)))\n",
        "  print(\"Number of Nan :{}\".format(yacht.isna().sum().sum())) \n",
        "  return yacht, 7, \"Yacht Hydrodynamics\"\n",
        "\n",
        "def year_prediction():\n",
        "  year = [str(i) for i in range(90)]\n",
        "  year_prediction = pd.read_csv(year_prediction_url, sep=\" \", names = year, header=None)\n",
        "  print(\"Year_prediction_msd dataset - {} examples - 90 columns\".format(len(year_prediction)))\n",
        "  print(\"Number of Nan :{}\".format(year_prediction.isna().sum().sum())) \n",
        "  return year_prediction, 90, \"Year Prediction MSD\"\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIKIJsh8lp1n",
        "colab_type": "text"
      },
      "source": [
        "Here you can select the dataset you want to load in by selecting the appropriate function. The rest of the code is going to split a training and test set, and then put the data in cuda tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O47F8PUHSQn0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "246d04b3-48c8-4fd4-8ed0-94eced92848f"
      },
      "source": [
        "## Loading data: the interface is simple, every dataset can be load and preprocessed by calling the function\n",
        "# with its name .\n",
        "\n",
        "# 1 - Call the dataset you want, this is the only thing you need to choose\n",
        "data, nb_features, name = year_prediction()\n",
        "\n",
        "\n",
        "# 2 - Preprocessing stuff ...\n",
        "nb_features = nb_features-1\n",
        "col = str(nb_features) \n",
        "x = data.drop(col, axis=1)\n",
        "y = data[col]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
        "\n",
        "x_train = np.array(x_train, dtype='float32')\n",
        "y_train = np.array(y_train, dtype='float32')\n",
        "x_test = np.array(x_test, dtype='float32')\n",
        "y_test = np.array(y_test, dtype='float32')\n",
        "\n",
        "x_train_tensor = torch.tensor(np.array(x_train),device = device)\n",
        "y_train_tensor = torch.tensor(np.array(y_train),device = device)\n",
        "x_test_tensor = torch.tensor(np.array(x_test),device = device)\n",
        "y_test_tensor = torch.tensor(np.array(y_test),device = device)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Year_prediction_msd dataset - 515345 examples - 90 columns\n",
            "Number of Nan :0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpla0QJamGrT",
        "colab_type": "text"
      },
      "source": [
        "#Model Definition\n",
        "We define a 3 hidden layer neural net using the Scaled Exponential Linear Unit  activation function. We also add a residual connection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQu2wKBcSXAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(NN, self).__init__()\n",
        "        self.L1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.L2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.L3 = nn.Linear(hidden_dim, output_dim, bias = False)\n",
        "        \n",
        "    \n",
        "    def forward(self,x):\n",
        "        # return x.w + b\n",
        "        h1 = F.selu(self.L1(x)) \n",
        "        h2 = F.selu(self.L2(h1)) + h1\n",
        "        out = self.L3(h2)        \n",
        "        return out\n",
        "             \n",
        "pyro.clear_param_store()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D2SuV6AmvCn",
        "colab_type": "text"
      },
      "source": [
        "Now we define the model and the guide for variational inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FLQyIj6Sa93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### BAYESIAN LINEAR REGRESSION ############### \n",
        "\n",
        "pi = .5\n",
        "sigma1 = 1\n",
        "sigma2 = np.exp(-6)\n",
        "hidden_dim = 100\n",
        "\n",
        "neural_net = NN(nb_features,hidden_dim,1).cuda()   \n",
        "\n",
        "def model(x_data, y_data, hidden_dim):\n",
        "    # Prior distributions\n",
        "    # Setting up the fixed parameter in our scaled mixture of gaussian\n",
        "    components_logits = torch.tensor([pi,1-pi], device=device)\n",
        "    component_scale = torch.tensor([sigma1,sigma2], device=device)\n",
        "\n",
        "    loc_features_input = torch.zeros(2,nb_features,device=device)           \n",
        "    coord_scale_input = torch.cat((torch.ones(1,nb_features,device=device) * sigma1,torch.ones(1,nb_features,device=device) * sigma2),0)\n",
        "    \n",
        "    loc_features_hidden = torch.zeros(2,hidden_dim,device=device)\n",
        "    coord_scale_hidden = torch.cat((torch.ones(1,hidden_dim,device=device) * sigma1,torch.ones(1,hidden_dim,device=device) * sigma2),0)\n",
        "        \n",
        "    # 1st layer \n",
        "    scale_gaussian_w1 = MixtureOfDiagNormals(loc_features_input, coord_scale_input, components_logits).expand([hidden_dim]).to_event(1)  \n",
        "    scale_gaussian_b1 = MixtureOfDiagNormals(loc_features_hidden, coord_scale_hidden, components_logits)  \n",
        "   \n",
        "    # 2nd layer \n",
        "    scale_gaussian_w2 = MixtureOfDiagNormals(loc_features_hidden, coord_scale_hidden, components_logits).expand([hidden_dim]).to_event(1)\n",
        "    scale_gaussian_b2 = MixtureOfDiagNormals(loc_features_hidden, coord_scale_hidden, components_logits)  \n",
        "\n",
        "    # 3rd layer weight  \n",
        "    scale_gaussian_w3 = MixtureOfDiagNormals(loc_features_hidden, coord_scale_hidden, components_logits).expand([1]).to_event(1)\n",
        "\n",
        "    priors = {'L1.weight': scale_gaussian_w1, 'L1.bias': scale_gaussian_b1,\n",
        "              'L2.weight' :scale_gaussian_w2, 'L2.bias':scale_gaussian_b2,\n",
        "              'L3.weight' :scale_gaussian_w3}\n",
        "    \n",
        "    lifted_module = pyro.random_module(\"module\", neural_net, priors)\n",
        "    # sample a nn (which also samples w and b)\n",
        "    lifted_reg_model = lifted_module()\n",
        "    # Assigning priors to weights\n",
        "    with pyro.plate(\"map\", size=len(x_data)):\n",
        "        # run the nn forward on data\n",
        "        prediction_mean = lifted_reg_model(x_data).squeeze(-1)\n",
        "        # condition on the observed data\n",
        "        pyro.sample(\"obs\",\n",
        "                    Normal(prediction_mean, 1),\n",
        "                    obs=y_data)\n",
        "        return prediction_mean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0G7vMouSdJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def guide(x_data, y_data, hidden_dim):    \n",
        "    # Global initilizer\n",
        "    w_input = torch.rand(hidden_dim, nb_features,device=device)\n",
        "    w_hidden = torch.rand(hidden_dim, hidden_dim, device=device)\n",
        "    w_output = torch.rand(1, hidden_dim, device=device)    \n",
        "    b = torch.rand(hidden_dim, device=device)\n",
        "    \n",
        "    # Variational posterior approximation\n",
        "    # Layer 1 \n",
        "    w1_mu = pyro.param(\"w1_mu\", w_input)\n",
        "    w1_sigma = pyro.param(\"w1_sigma\", w_input, constraint=constraints.positive)\n",
        "    b1_mu = pyro.param(\"b1_mu\", b)\n",
        "    b1_sigma = pyro.param(\"b1_sigma\", b, constraint=constraints.positive)\n",
        "    \n",
        "    # Layer 2 \n",
        "    w2_mu = pyro.param(\"w2_mu\", w_hidden)\n",
        "    w2_sigma = pyro.param(\"w2_sigma\", w_hidden, constraint=constraints.positive)    \n",
        "    b2_mu = pyro.param(\"b2_mu\", b)\n",
        "    b2_sigma = pyro.param(\"b2_sigma\", b, constraint=constraints.positive)\n",
        "        \n",
        "\n",
        "    # Layer 10 weight\n",
        "    w3_mu = pyro.param(\"w3_mu\", w_output)\n",
        "    w3_sigma = pyro.param(\"w3_sigma\", w_output, constraint=constraints.positive)\n",
        "    \n",
        "    # Latent variables\n",
        "    w1 = Normal(w1_mu, w1_sigma).to_event(2)   \n",
        "    b1 = Normal(b1_mu, b1_sigma).to_event(1)    \n",
        "    \n",
        "    w2 = Normal(w2_mu, w2_sigma).to_event(2)    \n",
        "    b2 = Normal(b2_mu, b2_sigma).to_event(1) \n",
        "    \n",
        "    w3 = Normal(w3_mu, w3_sigma).to_event(2)   \n",
        "    \n",
        "    params = {'L1.weight': w1, 'L1.bias': b1,\n",
        "              'L2.weight' :w2, 'L2.bias':b2,\n",
        "              'L3.weight' :w3}\n",
        "    # overloading the parameters in the module with random samples from the guide distributions\n",
        "    lifted_module = pyro.random_module(\"module\", neural_net, params)\n",
        "    # sample a regressor\n",
        "    return lifted_module()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ldeDkY5m-Yg",
        "colab_type": "text"
      },
      "source": [
        "#Hyperparameter search\n",
        "We first run the model for 5 epochs to get a baseline score before appliying bayesian optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIUzISkvUkQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "optimizer = Adam({\"lr\": 0.005})\n",
        "svi = SVI(model, guide, optimizer, loss=Trace_ELBO(), num_samples=100)\n",
        "pyro.clear_param_store()\n",
        "\n",
        "\n",
        "num_iterations = 5\n",
        "def train(num_iterations):\n",
        "    pyro.clear_param_store()\n",
        "    for j in range(num_iterations):\n",
        "        # calculate the loss and take a gradient step\n",
        "        loss = svi.step(x_train_tensor, y_train_tensor, hidden_dim)        \n",
        "\n",
        "train(num_iterations)            \n",
        "baseline_score =  svi.step(x_train_tensor, y_train_tensor, hidden_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOIxRzQ9nO_Q",
        "colab_type": "text"
      },
      "source": [
        "We then define the function to optimize. Basically we want to reduce the loss after 5 epochs, depending on learning rate and the hidden dimension. We are going to search the hidden dimention between 10 and 100 units, and the learning rate between 0.0001 and 0.01. Finaly we use the Maximum Probability of Improvement as the acquisition function in the Gaussian process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmvfugGIXLb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_to_optimize(x):\n",
        "  \"\"\"\n",
        "  Function to optimize with Bayesian Optimization (BO)\n",
        "  we basically run our model with different learning rate and hidden dimension.\n",
        "  \"\"\"\n",
        "  learning_rate = float(x[:,0])\n",
        "  hidden_dim = int(x[:,1])\n",
        "\n",
        "  neural_net = NN(nb_features,hidden_dim,1).cuda()   \n",
        "  optimizer = Adam({\"lr\": learning_rate})\n",
        "  svi = SVI(model, guide, optimizer, loss=Trace_ELBO(), num_samples=100)\n",
        "  pyro.clear_param_store()\n",
        "  # Training\n",
        "  num_iterations = 5\n",
        "  def train(num_iterations):\n",
        "      pyro.clear_param_store()\n",
        "      for j in range(num_iterations):\n",
        "          # calculate the loss and take a gradient step\n",
        "          loss = svi.step(x_train_tensor, y_train_tensor, hidden_dim)\n",
        "\n",
        "          train(num_iterations)\n",
        "  # Evaluating the trained model after 5 epochs      \n",
        "  loss =  svi.step(x_train_tensor, y_train_tensor, hidden_dim)  \n",
        "  return loss\n",
        "\n",
        "\n",
        "bounds = [\n",
        "            {'name': 'learning_rate', 'type': 'continuous', 'domain': (10**-4,10**-2)},\n",
        "            {'name': 'hidden_dim', 'type': 'discrete', 'domain': np.arange(10,100)},\n",
        "         ]\n",
        "\n",
        "optimizer = GPyOpt.methods.BayesianOptimization(f=model_to_optimize, domain=bounds,\n",
        "                                                        acquisition_type = 'MPI',\n",
        "                                                        acquisition_par = 0.1,                                                    \n",
        "                                                        model_type='GP')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md_BLHXrox3Q",
        "colab_type": "text"
      },
      "source": [
        "We run Bayesian optimization for 20 iteration and display the found hyperparameters and the performance boost when comparing with the baseline score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eHNC7_VuU5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "b98d569e-60c4-4a69-e536-eda8c044edbb"
      },
      "source": [
        "optimizer.run_optimization(max_iter=20, eps=-1)\n",
        "\n",
        "## Bayesian Optimization Evaluation\n",
        "\n",
        "best_hyperparameters = optimizer.X[np.argmin(optimizer.Y)]\n",
        "performance_boost_20_run = (baseline_score/np.min(optimizer.Y) -1)*100\n",
        "print(\"best learning rate: {}\".format(best_hyperparameters[0]))\n",
        "print(\"best hidden dimension: {}\".format(best_hyperparameters[1]))\n",
        "print(\"The performance boost after 20 iteration is : {} %\".format(np.floor(performance_boost_20_run)))\n",
        "\n",
        "# Plot the optimizer convergence\n",
        "# two graphs, the first describe the distance between a set of hyperparameters\n",
        "# at a given evaluation to another; the second graph shows an evolution\n",
        "# of the value of the lowest loss we found.\n",
        "optimizer.plot_convergence()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best learning rate: 0.01\n",
            "best hidden dimension: 47.0\n",
            "The performance boost after 20 iteration is : 4578.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAFNCAYAAACjRAOYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJzSFL0WpZFA",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEFIWDEquYDN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3bbda26-e6c9-42cf-ffda-a0e705710d8d"
      },
      "source": [
        "## Training with optimal hyperparameters\n",
        "optimizer = Adam({\"lr\": best_hyperparameters[0]})\n",
        "svi = SVI(model, guide, optimizer, loss=Trace_ELBO(), num_samples=100)\n",
        "pyro.clear_param_store()\n",
        "\n",
        "hidden_dim = int(best_hyperparameters[1])\n",
        "neural_net = NN(nb_features,hidden_dim,1).cuda()   \n",
        "\n",
        "num_iterations = 35000\n",
        "losses = []\n",
        "batch_size = len(x_train_tensor)\n",
        "def train(num_iterations):\n",
        "    pyro.clear_param_store()\n",
        "    for j in range(num_iterations):\n",
        "        # calculate the loss and take a gradient step\n",
        "        loss = svi.step(x_train_tensor, y_train_tensor, hidden_dim)\n",
        "        losses.append(loss)\n",
        "        if j % 500 == 0:\n",
        "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / batch_size))\n",
        "\n",
        "train(num_iterations)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[iteration 0001] loss: 6360252561425.7539\n",
            "[iteration 0501] loss: 8268386176.4009\n",
            "[iteration 1001] loss: 10242549220.8405\n",
            "[iteration 1501] loss: 2472241007.7718\n",
            "[iteration 2001] loss: 246791252.1601\n",
            "[iteration 2501] loss: 138727692.2510\n",
            "[iteration 3001] loss: 101386485.8641\n",
            "[iteration 3501] loss: 148123553.5715\n",
            "[iteration 4001] loss: 38825309.6086\n",
            "[iteration 4501] loss: 104088724.1601\n",
            "[iteration 5001] loss: 33752721.1910\n",
            "[iteration 5501] loss: 189221628.6446\n",
            "[iteration 6001] loss: 41391462.0428\n",
            "[iteration 6501] loss: 20588144.6569\n",
            "[iteration 7001] loss: 19488951.8784\n",
            "[iteration 7501] loss: 11101654.0050\n",
            "[iteration 8001] loss: 7054717.1986\n",
            "[iteration 8501] loss: 5702544.5236\n",
            "[iteration 9001] loss: 3300708.6922\n",
            "[iteration 9501] loss: 10853907.8327\n",
            "[iteration 10001] loss: 5386587.5565\n",
            "[iteration 10501] loss: 2200526.5367\n",
            "[iteration 11001] loss: 3244207.6484\n",
            "[iteration 11501] loss: 2416597.3766\n",
            "[iteration 12001] loss: 5135888.2300\n",
            "[iteration 12501] loss: 2748739.8893\n",
            "[iteration 13001] loss: 2000376.8152\n",
            "[iteration 13501] loss: 2192434.0448\n",
            "[iteration 14001] loss: 1826760.0678\n",
            "[iteration 14501] loss: 1031363.3521\n",
            "[iteration 15001] loss: 916468.2679\n",
            "[iteration 15501] loss: 325104.3963\n",
            "[iteration 16001] loss: 919261.4706\n",
            "[iteration 16501] loss: 1938293.0453\n",
            "[iteration 17001] loss: 545683.9720\n",
            "[iteration 17501] loss: 831352.3499\n",
            "[iteration 18001] loss: 172110.0989\n",
            "[iteration 18501] loss: 99021.5780\n",
            "[iteration 19001] loss: 874061.8671\n",
            "[iteration 19501] loss: 1347927.8657\n",
            "[iteration 20001] loss: 1493233.3492\n",
            "[iteration 20501] loss: 159858.2996\n",
            "[iteration 21001] loss: 378724.2112\n",
            "[iteration 21501] loss: 654812.2043\n",
            "[iteration 22001] loss: 16412.2786\n",
            "[iteration 22501] loss: 39658.7360\n",
            "[iteration 23001] loss: 113870.0145\n",
            "[iteration 23501] loss: 133518.7653\n",
            "[iteration 24001] loss: 67454.1607\n",
            "[iteration 24501] loss: 749409.3442\n",
            "[iteration 25001] loss: 101177.0564\n",
            "[iteration 25501] loss: 286255.6158\n",
            "[iteration 26001] loss: 10644.5236\n",
            "[iteration 26501] loss: 107787.9913\n",
            "[iteration 27001] loss: 20029.1181\n",
            "[iteration 27501] loss: 312.8804\n",
            "[iteration 28001] loss: 5146.6796\n",
            "[iteration 28501] loss: 1100.8787\n",
            "[iteration 29001] loss: 596.4414\n",
            "[iteration 29501] loss: 4301.0815\n",
            "[iteration 30001] loss: 287.9859\n",
            "[iteration 30501] loss: 1244.7869\n",
            "[iteration 31001] loss: 5387.1994\n",
            "[iteration 31501] loss: 1128.2439\n",
            "[iteration 32001] loss: 197.9253\n",
            "[iteration 32501] loss: 3924.7722\n",
            "[iteration 33001] loss: 892.6314\n",
            "[iteration 33501] loss: 1197.8327\n",
            "[iteration 34001] loss: 1369.2128\n",
            "[iteration 34501] loss: 417.9033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Od2CGMDvd5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "7bc4f273-185f-4fec-fd55-4dc25a47548d"
      },
      "source": [
        "# Displaying the ELBO given epochs\n",
        "len_loss = len(losses)\n",
        "print(\"loss length: {}\".format(len_loss))\n",
        "cost = np.array(losses)    \n",
        "t = np.arange(len_loss)    \n",
        "plt.plot(t,cost)"
      ],
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss length: 35000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3200f2ff60>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 352
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFbBJREFUeJzt3X20ZXV93/H3hxnABwgPzlVZPDho\noAmkPuCVmMZYmsQ6YBa0q9pAmvpQzKzViE2bpBWXLVps14rStK6soGRiCNEmID7ETuMYNNEUVwzI\nJcDIwwInA8ogODc8qogw8O0fZ188c+eee87ce+69Z2/fr7Xumr1/+3f2/p49Zz6z72/vs3eqCklS\ntxyw1gVIksbPcJekDjLcJamDDHdJ6iDDXZI6yHCXpA5a03BPcmmS3UluHqHvq5P8bZI9SV4/b9lx\nST6X5LYktybZuFI1S1IbrPWR+2XAphH7fgN4M/AnCyz7CHBRVf04cCqwexzFSVJbrWm4V9XVwAP9\nbUlelOTPk1yf5EtJfqzpe1dVbQeemtf/JGB9VX2+6fedqnp0ld6CJE2ktT5yX8gW4O1V9XLgN4EP\nDul/IvBQkk8luSHJRUnWrXiVkjTB1q91Af2SHAL8I+DjSeaaDx7ysvXAzwAvozd08zF6wzd/sDJV\nStLkm6hwp/ebxENV9dL9eM0u4Maq2gmQ5NPAKzHcJf0Qm6hhmap6BLgzyRsA0vOSIS+7Djg8yVQz\n/7PArStYpiRNvKzlXSGTXA6cBmwAvgW8G/gC8CHgKOBA4IqqujDJK4A/BY4AHgPuq6qTm/W8Bvht\nIMD1wOaqenx1340kTY41DXdJ0sqYqGEZSdJ4rNkJ1Q0bNtTGjRvXavOS1ErXX3/931fV1LB+axbu\nGzduZGZmZq02L0mtlOTro/RzWEaSOshwl6QOMtwlqYMMd0nqIMNdkjrIcJekDhoa7qM8LSnJaUlu\nTHJLkv833hIlSftrlCP3y1jkaUlJDqd3z/Uzm3u9vGE8pS1sz5NPceV1d/PkU942QZIGGRruCz0t\naZ5fAj5VVd9o+q/oI+4+8jdf5z99cjt/fO1I1/FL0g+lcYy5nwgckeSvmkfjvXFQxySbk8wkmZmd\nnV3Sxh58tHezx4cefWJJr5ekHwbjCPf1wMuB1wGvBf5LkhMX6lhVW6pquqqmp6aG3hpBkrRE47i3\nzC7g/qr6LvDdJFcDLwHuGMO6JUlLMI4j9/8DvCrJ+iTPAn4SuG0M65UkLdHQI/f+pyUl2UXvaUkH\nAlTVJVV1W5I/B7YDTwEfrqqBl01Kklbe0HCvqnNG6HMRcNFYKhq6rdXYiiS1W2u/oZq1LkCSJlhr\nw12SNJjhLkkdZLhLUgcZ7pLUQa0L98LLZSRpmNaF+5x4uYwkDdTacJckDWa4S1IHGe6S1EGGuyR1\nUOvC3XvLSNJwrQv3OfFyGUkaqLXhLkkazHCXpA4aGu5JLk2yO8miD+BI8ooke5K8fnzlSZKWYpQj\n98uATYt1SLIOeB/wuTHUJElapqHhXlVXAw8M6fZ24JPA7nEUtWg9K70BSeqAZY+5Jzka+OfAh5Zf\njiRpHMZxQvUDwDuq6qlhHZNsTjKTZGZ2dnYMm5YkLWToA7JHMA1c0Vx3vgE4I8meqvr0/I5VtQXY\nAjA9Pe0IiyStkGWHe1UdPzed5DLgzxYKdknS6hka7kkuB04DNiTZBbwbOBCgqi5Z0eokSUsyNNyr\n6pxRV1ZVb15WNSNtY6W3IEnt19pvqHprGUkarLXhLkkazHCXpA4y3CWpgwx3Seqg1oV7eXcZSRqq\ndeE+J3i5jCQN0tpwlyQNZrhLUgcZ7pLUQe0Ld8+nStJQ7Qv3hrcfkKTBWhvukqTBDHdJ6iDDXZI6\nyHCXpA4aGu5JLk2yO8nNA5b/qyTbk3w1yZeTvGT8Zf6AF8tI0nCjHLlfBmxaZPmdwD+uqn8IvJfm\nAdgrzYtlJGmwUR6zd3WSjYss/3Lf7DXAMcsvS5K0HOMecz8X+OyghUk2J5lJMjM7OzvmTUuS5owt\n3JP8E3rh/o5BfapqS1VNV9X01NTUuDYtSZpn6LDMKJK8GPgwcHpV3T+OdUqSlm7ZR+5JjgM+Bfzr\nqrpj+SUtrsrrZSRpmKFH7kkuB04DNiTZBbwbOBCgqi4BLgCeA3wwvRu+7Kmq6ZUq+Ad1rfQWJKm9\nRrla5pwhy98KvHVsFUmSls1vqEpSBxnuktRBhrskdVDrwt2LZSRpuNaF+5x4dxlJGqi14S5JGsxw\nl6QOMtwlqYMMd0nqoNaFuxfLSNJwrQv3Od5bRpIGa224S5IGM9wlqYMMd0nqIMNdkjqodeHuvWUk\nabih4Z7k0iS7k9w8YHmS/E6SHUm2Jzll/GVKkvbHKEfulwGbFll+OnBC87MZ+NDyy5IkLcfQcK+q\nq4EHFulyFvCR6rkGODzJUeMqUJK0/8Yx5n40cHff/K6mbR9JNieZSTIzOzs7hk1LkhayqidUq2pL\nVU1X1fTU1NRqblqSfqiMI9zvAY7tmz+maVsR5d1lJGmocYT7VuCNzVUzrwQerqp7x7DeRcWby0jS\nQOuHdUhyOXAasCHJLuDdwIEAVXUJsA04A9gBPAq8ZaWKlSSNZmi4V9U5Q5YX8LaxVSRJWrbWfUNV\nkjSc4S5JHdS6cPfeMpI0XOvCfY7XykjSYK0Nd0nSYIa7JHWQ4S5JHWS4S1IHGe6S1EGtDXdvLSNJ\ng7U23CVJgxnuktRBrQ13v6kqSYO1NtwlSYMZ7pLUQSOFe5JNSW5PsiPJ+QssPy7JF5PckGR7kjPG\nX+r8ba70FiSpvYaGe5J1wMXA6cBJwDlJTprX7T8DV1bVy4CzgQ+Ou1BJ0uhGOXI/FdhRVTur6nHg\nCuCseX0K+JFm+jDgm+MrUZK0v0YJ96OBu/vmdzVt/d4D/HLzjNVtwNsXWlGSzUlmkszMzs4uoVxJ\n0ijGdUL1HOCyqjqG3sOyP5pkn3VX1Zaqmq6q6ampqTFtWpI03yjhfg9wbN/8MU1bv3OBKwGq6m+A\nZwAbxlHgfOUF7pI01Cjhfh1wQpLjkxxE74Tp1nl9vgH8HECSH6cX7is67uLFMpI02NBwr6o9wHnA\nVcBt9K6KuSXJhUnObLr9BvArSW4CLgfeXB5iS9KaWT9Kp6raRu9EaX/bBX3TtwI/Pd7SJElL5TdU\nJamDDHdJ6qDWhbsD+ZI0XOvCfU68uYwkDdTacJckDWa4S1IHGe6S1EGGuyR1UOvC3e+9StJwrQv3\nOV4sI0mDtTbcJUmDGe6S1EGGuyR1kOEuSR3UunD/B88/FIDnPPvgNa5EkiZX68L9RVOHAHDksw9a\n40okaXKNFO5JNiW5PcmOJOcP6PMvk9ya5JYkfzLeMiVJ+2Pok5iSrAMuBl4D7AKuS7K1efrSXJ8T\ngHcCP11VDyZ57koVLEkabpQj91OBHVW1s6oeB64AzprX51eAi6vqQYCq2j3eMiVJ+2OUcD8auLtv\nflfT1u9E4MQkf53kmiSbFlpRks1JZpLMzM7OLq1iSdJQ4zqhuh44ATgNOAf4/SSHz+9UVVuqarqq\npqempsa0aUnSfKOE+z3AsX3zxzRt/XYBW6vqiaq6E7iDXthLktbAKOF+HXBCkuOTHAScDWyd1+fT\n9I7aSbKB3jDNzjHWuY/H9jy5kquXpFYbGu5VtQc4D7gKuA24sqpuSXJhkjObblcB9ye5Ffgi8B+r\n6v6VKhrgLX943UquXpJabeilkABVtQ3YNq/tgr7pAn69+ZEkrbHWfUNVkjSc4S5JHWS4S1IHGe6S\n1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUge1Ltwff/KptS5BkiZe68L9Gw88utYl\nSNLEa124S5KGM9wlqYMMd0nqoJHCPcmmJLcn2ZHk/EX6/YsklWR6fCVKkvbX0HBPsg64GDgdOAk4\nJ8lJC/Q7FPg14NpxFylJ2j+jHLmfCuyoqp1V9ThwBXDWAv3eC7wPeGyM9UmSlmCUcD8auLtvflfT\n9rQkpwDHVtVnFltRks1JZpLMzM7O7nexkqTRLPuEapIDgP8J/MawvlW1paqmq2p6ampquZuWJA0w\nSrjfAxzbN39M0zbnUOAngL9KchfwSmCrJ1Ulae2MEu7XASckOT7JQcDZwNa5hVX1cFVtqKqNVbUR\nuAY4s6pmVqLgrMRKJaljhoZ7Ve0BzgOuAm4DrqyqW5JcmOTMlS5QkrT/1o/Sqaq2AdvmtV0woO9p\nyy9LkrQcfkNVkjrIcJekDjLcJamDWhfutdYFSFILtC7cJUnDGe6S1EGtC3e/xCRJw7Uu3CVJwxnu\nktRBhrskdVDrwn3n7HfXugRJmnitC/cvfc2HfEjSMK0L93i5jCQN1bpwlyQN17pwP8BDd0kaaqRw\nT7Ipye1JdiQ5f4Hlv57k1iTbk/xlkheMv1RJ0qiGhnuSdcDFwOnAScA5SU6a1+0GYLqqXgx8Anj/\nuAvtq2elVi1JnTHKkfupwI6q2llVjwNXAGf1d6iqL1bVo83sNfQeor0ijHZJGm6UcD8auLtvflfT\nNsi5wGcXWpBkc5KZJDOzs17SKEkrZawnVJP8MjANXLTQ8qraUlXTVTU9NTW17O099sSTy16HJHXR\nKOF+D3Bs3/wxTdtekvw88C7gzKr6/njK21f/kPv3HjfcJWkho4T7dcAJSY5PchBwNrC1v0OSlwG/\nRy/Yd4+/TEnS/hga7lW1BzgPuAq4Dbiyqm5JcmGSM5tuFwGHAB9PcmOSrQNWt2z9R+4+ck+SFrZ+\nlE5VtQ3YNq/tgr7pnx9zXZKkZWj1N1SrPHaXpIW0Lty9zl2ShmtduHtbSEkarn3h3ufl/+0v2P3t\nx9a6DEmaOK0Od4Cb7n54rUuQpInTunCfPyjjSVVJ2lf7wn1euj9ltkvSPloX7nfc9+15Laa7JM3X\nunA/+ejD9pr3yF2S9tW6cN93zH1NypCkida+cJ+X7vc94qWQkjRf68J9vv9x1e1rXYIkTZzWhfvu\nb+99q/jv+cAOSdpH68J99pF9nwPite6StLfWhfvLXnDEPm2X/vVdq1+IJE2w1oX7QrcNe++f3cqX\nd/z9qtciSZNqpHBPsinJ7Ul2JDl/geUHJ/lYs/zaJBvHXeicpwYMwfzSh69l603f5LZ7H1mpTUtS\nawx9ElOSdcDFwGuAXcB1SbZW1a193c4FHqyqH01yNvA+4BdXouDDn3XQwGX/7vIb9po/9shnct/D\nj/FPT34+0y84gvu/8zg/+txDeO3Jz2fdAb3fAdYfEA5opquKJFQVVT+47DJ9119+f8+TPPFk8awD\n1z39OkmaNKM8Zu9UYEdV7QRIcgVwFtAf7mcB72mmPwH8bpLUCpzpfONPvYD/e9M3R+p79wPfA+Az\n2+/lM9vvXfa2D3vmgTz8vSf2ajvhuYcse72Sfrj84iuO5a0/88IV3cYo4X40cHff/C7gJwf1qao9\nSR4GngPsNRCeZDOwGeC4445bUsGv2Hjkkl43Ds899OC9wv25hx7MCc8z3CXtnw2HHLzi2xjpAdnj\nUlVbgC0A09PTSz6qv+u3Xje2miSpi0Y5oXoPcGzf/DFN24J9kqwHDgPuH0eBkqT9N0q4XweckOT4\nJAcBZwNb5/XZCrypmX498IWVGG+XJI1m6LBMM4Z+HnAVsA64tKpuSXIhMFNVW4E/AD6aZAfwAL3/\nACRJa2SkMfeq2gZsm9d2Qd/0Y8AbxluaJGmpWvcNVUnScIa7JHWQ4S5JHWS4S1IHZa2uWEwyC3x9\niS/fwLxvv064NtXbplqhXfW2qVZoV71tqhWWV+8LqmpqWKc1C/flSDJTVdNrXceo2lRvm2qFdtXb\nplqhXfW2qVZYnXodlpGkDjLcJamD2hruW9a6gP3UpnrbVCu0q9421QrtqrdNtcIq1NvKMXdJ0uLa\neuQuSVqE4S5JHdS6cB/2sO5VrOOuJF9NcmOSmabtyCSfT/K15s8jmvYk+Z2m5u1JTulbz5ua/l9L\n8qZB21tCfZcm2Z3k5r62sdWX5OXN+9/RvHbJD5QdUOt7ktzT7N8bk5zRt+ydzXZvT/LavvYFPxvN\n7aqvbdo/1ty6eqm1Hpvki0luTXJLkl9r2id13w6qd+L2b5JnJPlKkpuaWv/rYutPcnAzv6NZvnGp\n72HM9V6W5M6+ffvSpn11Pwu9h0G344feLYf/DnghcBBwE3DSGtVyF7BhXtv7gfOb6fOB9zXTZwCf\nBQK8Eri2aT8S2Nn8eUQzfcSY6ns1cApw80rUB3yl6ZvmtaePudb3AL+5QN+Tmr/3g4Hjm8/DusU+\nG8CVwNnN9CXAv11GrUcBpzTThwJ3NDVN6r4dVO/E7d/m/R7STB8IXNvshwXXD/wqcEkzfTbwsaW+\nhzHXexnw+gX6r+pnoW1H7k8/rLuqHgfmHtY9Kc4C/qiZ/iPgn/W1f6R6rgEOT3IU8Frg81X1QFU9\nCHwe2DSOQqrqanr31h97fc2yH6mqa6r3CfxI37rGVesgZwFXVNX3q+pOYAe9z8WCn43mSOdn6T24\nff77Xkqt91bV3zbT3wZuo/cM4Undt4PqHWTN9m+zj77TzB7Y/NQi6+/f558Afq6pZ7/ew1JqHVLv\nIKv6WWhbuC/0sO7FPqgrqYDPJbk+vQd/Azyvqu5tpu8DntdMD6p7td/PuOo7upme3z5u5zW/vl46\nN8yxhFqfAzxUVXvGXWszDPAyekdsE79v59ULE7h/k6xLciOwm17I/d0i63+6pmb5w009q/bvbX69\nVTW3b/97s2//V5K5p2Gv6mehbeE+SV5VVacApwNvS/Lq/oXN/7QTe53ppNcHfAh4EfBS4F7gt9e2\nnL0lOQT4JPDvq+qR/mWTuG8XqHci929VPVlVL6X3rOZTgR9b45IWNb/eJD8BvJNe3a+gN9TyjrWo\nrW3hPsrDuldFVd3T/Lkb+FN6H8RvNb9K0fy5u+k+qO7Vfj/jqu+eZnp++9hU1beafzhPAb9Pb/8u\npdb76f36u35e+5IlOZBeUP5xVX2qaZ7YfbtQvZO8f5v6HgK+CPzUIut/uqZm+WFNPav+762v3k3N\nUFhV1feBP2Tp+3Z5n4VRB+cn4YfeYwF30jtJMndC5OQ1qOPZwKF901+mN1Z+EXufVHt/M/069j6R\n8pX6wYmUO+mdRDmimT5yjHVuZO+TlGOrj31P9Jwx5lqP6pv+D/TGUAFOZu+TZTvpnSgb+NkAPs7e\nJ+R+dRl1ht7Y5wfmtU/kvl2k3onbv8AUcHgz/UzgS8AvDFo/8Db2PqF65VLfw5jrPapv338A+K21\n+CysaiiO44feGec76I3FvWuNanhh88G4Cbhlrg56431/CXwN+Iu+v6AAFzc1fxWY7lvXv6F3wmcH\n8JYx1ng5vV+3n6A3VnfuOOsDpoGbm9f8Ls23ncdY60ebWrYDW9k7jN7VbPd2+q4eGPTZaP6+vtK8\nh48DBy+j1lfRG3LZDtzY/Jwxwft2UL0Tt3+BFwM3NDXdDFyw2PqBZzTzO5rlL1zqexhzvV9o9u3N\nwP/mB1fUrOpnwdsPSFIHtW3MXZI0AsNdkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA76/yqqx17C\nlBfrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw4C41DoP-8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(neural_net, 'neural_net_BbB_2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyi-FmXQxUlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "######### Evaluate the model\n",
        "# Those utility functions are going to be used further the line to evaluate the predictions.\n",
        "get_marginal = lambda traces, sites:EmpiricalMarginal(traces, sites)._get_samples_and_weights()[0].detach().cpu().numpy()\n",
        "\n",
        "def picp(y, y_lower, y_upper):\n",
        "    batch_size = len(y)\n",
        "    k = np.zeros(batch_size)\n",
        "    for i in range(batch_size):\n",
        "        k[i] = y_lower[i] <= y[i] <= y_upper[i]\n",
        "    return np.mean(k)\n",
        "\n",
        "def mpiw(y_lower, y_upper):\n",
        "    return np.mean(y_upper-y_lower)\n",
        "\n",
        "def wrapped_model(x_data, y_data, hidden_dim):\n",
        "    pyro.sample(\"prediction\", Delta(model(x_data, y_data, hidden_dim)))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSUrUtGl7WLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posterior = svi.run(x_test_tensor, y_test_tensor, hidden_dim)\n",
        "\n",
        "trace_pred = TracePredictive(wrapped_model,\n",
        "                             posterior,\n",
        "                             num_samples=1000)\n",
        "\n",
        "\n",
        "post_pred_test = trace_pred.run(x_test_tensor, None, hidden_dim)\n",
        "marginal_test = get_marginal(post_pred_test, sites= ['prediction', 'obs'])\n",
        "sampled_prediction_test = marginal_test[:,1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PFjv1Qi0qY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Split the evaluations\n",
        "marginal_site = pd.DataFrame(sampled_prediction_test).transpose()\n",
        "describe = partial(pd.Series.describe, percentiles=[.025, 0.5, 0.975])\n",
        "summary = marginal_site.apply(describe, axis=1)[[\"mean\", \"std\", \"2.5%\", \"50%\", \"97.5%\"]]\n",
        "\n",
        "y_test_lower = np.array(summary['2.5%'])\n",
        "y_test_upper = np.array(summary['97.5%'])\n",
        "y_test_mean =  np.array(summary['mean'])\n",
        "\n",
        "proba_coverage_test = picp(y_test, y_test_lower, y_test_upper)\n",
        "interval_predicted_test = mpiw(y_test_lower, y_test_upper)\n",
        "rmse_test = np.mean(np.sqrt(np.power((y_test_mean-y_test),2)))\n",
        "\n",
        "interval_def = np.max(y) - np.min(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI6ukTd5xm7s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "46a2daa0-caf5-4b62-fee8-9c597c36083e"
      },
      "source": [
        "print(\"{} dataset\".format(name))\n",
        "print(\"Prediction Interval Coverage Probability (PICP): {}\".format(proba_coverage_test))\n",
        "print(\"Mean Prediction Interval Width (MPIW): {}\".format(interval_predicted_test))\n",
        "print(\"Natural Interval Width of Data: {}\".format(interval_def))\n",
        "print(\"Root Mean Square Error (RMSE): {}\".format(rmse_test))\n"
      ],
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Concrete Strength dataset\n",
            "Prediction Interval Coverage Probability (PICP): 1.0\n",
            "Mean Prediction Interval Width (MPIW): 9479.054434826303\n",
            "Natural Interval Width of Data: 80.27\n",
            "Root Mean Square Error (RMSE): 124.916060280742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Daq6WafuxrYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}